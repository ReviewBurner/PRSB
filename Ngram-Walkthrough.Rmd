---
title: "Chimpanzee Gesture Ngrams"
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output:
  pdf_document: 
    toc: yes
    fig_width: 8
    fig_height: 6
    fig_caption: yes
    number_sections: yes
fontsize: 12pt
spacing: double
fig_caption: yes
indent: yes
geometry: margin=1in
mainfont: Calibri
sansfont: Calibri
monofont: Calibri
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

## Abstract
Abstract
Recent research has produced evidence for basic combinatorial abilities in the vocal systems of different animal species. Here, we investigate the structure of gesture sequences in Eastern chimpanzees (Pan troglodytes schweinfurthii) to detect whether gestural communication shows non-random combinations and how combinatorial rules influence predictability. As the parsing of signals into sequences is dependent on researcher decisions, we employ a multiverse approach, considering four different definitions of what constitutes a ‘sequence’ based on varying time thresholds. Our results indicate that sequences tend to be short (even with the most liberal time window) and that transitions between some gesture types occur more frequently than expected by chance, with some transitions showing significant association across all time windows. These transitions often involve repetition, suggesting persistence as a key aspect of chimpanzee gestural sequences. Information about previous gestures reduced uncertainty in predicting subsequent gestures. The order of gestures within sequences appears to be less critical than their cooccurrence, challenging assumptions based on the linear patterning derived from vocal communication. The findings highlight the importance of methodological choices in sequence definition and suggest that chimpanzee gestural communication is characterised by a mix of predictability and flexibility, with implications for understanding the evolution of complex communication systems.

# Information

The code provided below should enable researchers to replicate all results in this study. All functions are defined initially in this document, rather than standalone functions - use the Table of Content to navigate.

# Set options
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE, 
  warning = FALSE, 
  cache = TRUE,
  cache.lazy = FALSE)
options(digits = 7)
```

# Set Libraries

```{r libraries, cache=FALSE}
#devtools::load_all("~/GitHub/DeepSequence/R/")
library(tidyverse)
library(kableExtra)
library(ggplot2)
library(tidymodels)
library(tidytext)
library(discrim)
library(igraph)
library(pROC)
library(furrr)
library(gridExtra)
library(ggraph)
library(ggdendro)
library(infotheo)
library(rstatix)
library(keras)
set.seed(2807)
```


# Load Data

We have four datasets, each with a different time threshold for defining a sequence. We will load all datasets. They are consistently labelled as '_5sec' (5 second time threshold), '_rapid' (based on rapid sequence definitions), '_overlap' (overlapping sequences), and '_solitary' (solitary gestures).

```{r load_data, cache=TRUE}
# Load .RData files
load("Data/sequence_data_5sec.RData")
load("Data/sequence_data_rapid.RData")
load("Data/sequence_data_overlap.RData")
load("Data/sequence_data_solitary.RData")
```

# Markov transitions

## N-Grams

The following functions take the sequence objects, calculate the probabilities of elements transitioning into each other, and then bootstrap the data to check the stability of the conditional probabilities and their significance compared to random.

## Functions

### Calculate N-grams

This function will calculate the probabilities of AB, ABC, and ABCD.

```{r ngram_function}
  # Create n-grams
calculate_ngrams <- function(sequences, n = 2) {
  # Create a data frame with a column named 'sequences'
  sequences = data.frame(
    sequences = sequences
  )
  
  # Calculate n-grams from the 'sequences' column
  ngrams <- sequences %>%
    unnest_tokens(
      output = "ngram", 
      input = "sequences", 
      token = "ngrams", 
      n = n, 
      to_lower = FALSE
    ) %>% 
    drop_na() %>%                  # Remove rows with missing n-grams
    filter(!str_detect(ngram, 'NA')) # Remove n-grams containing 'NA'
      
  # Create a new column 'antecedent' by extracting the first (i-1) elements from each n-gram
  ngrams$antecedent <- 
    sapply(ngrams$ngram, function(x){
      paste((str_split(x, ' ') %>% unlist())[1:(n-1)], collapse = ',')
    })
  
  # Create a new column 'consequent' by extracting the i-th element from each n-gram
  ngrams$consequent <- 
    sapply(ngrams$ngram, function(x){
      (str_split(x, ' ') %>% unlist())[n]
    })
  
  # Select and rearrange columns in the 'ngrams' data frame
  ngrams <- ngrams %>% 
    select(ngram, antecedent, consequent) %>%   # Select specific columns
    add_count(ngram, name = 'count') %>%   # Count
    add_count(antecedent, name = 'antecedent_count') %>%   # Count occurrences of antecedents
    add_count(consequent, name = 'consequent_count') %>%   # Count occurrences of consequents
    distinct(.keep_all = T) %>% 
    mutate(conditional_probability = count/antecedent_count) %>%   # Calculate conditional probability
    mutate(specificity = count/consequent_count)   # Calculate specificity
  
  return(ngrams)
}

```


### Bootstraps

This function bootstraps the n-grams to check for data quality.

```{r bootstrap_ngram_function}

bootstrap_n_grams_function <- function(observed_n_grams, sequences, n_bootstraps = 100) {
  lapply(observed_n_grams, function(y) {
    # Select the 'ngram' column from 'y' and determine its level by counting spaces
    observed_order <- y %>% select(ngram)
    level <- str_count(observed_order$ngram[[1]], ' ') + 1

    # Initialize an empty list 'boot'
    boot <- lapply(1:n_bootstraps, function(k) {
      # Create a bootstrapped n-gram set
      boot_gram <- calculate_ngrams(
        sequences = sample(sequences, replace = TRUE),
        n = level
      )
      
      # Join the observed order with the bootstrapped n-grams
      ordered_boot_gram <- left_join(observed_order, boot_gram, by = 'ngram') %>%
        replace_na(list('conditional_probability' = 0)) %>%
        pull(conditional_probability)
    }) %>% bind_cols() %>% suppressMessages()
  })
}

create_plots_function <- function(observed_n_grams, bootstrap_n_grams) {
  lapply(seq_along(observed_n_grams), function(y) {
    # Create a data frame with counts, range, and range difference
    data.frame(count = observed_n_grams[[y]]$antecedent_count,
               t(apply(bootstrap_n_grams[[y]], 1, range))) %>% 
      mutate(range = X2 - X1) %>% 
      ggplot(aes(x = count, y = range)) +
      geom_point(alpha = 0.3) + 
      theme_bw() +
      ylim(0, 1) + 
      xlab('Observed Antecedent Count') + 
      ylab('Range of Conditional Probabilities') + 
      ggtitle(y + 1)  # Set the plot title based on the iteration
  })
}

```


### Calculate 2,3, and 4-Grams

Here, we apply the n-gram function to the dataset.

```{r calculate_ngrams}
# 2-Grams
observed_n_grams_5sec <- 
  lapply(2:4, function(y){
    calculate_ngrams(sequences = sequences_5sec, 
                     n = y)
    }
  )

observed_n_grams_rapid <- 
  lapply(2:3, function(y){
    calculate_ngrams(sequences = sequences_rapid, n = y)
    }
  )

observed_n_grams_overlap <- 
  lapply(2:3, function(y){
    calculate_ngrams(sequences = sequences_overlap, n = y)
    }
  )

observed_n_grams_solitarygestures <- 
  lapply(2:3, function(y){
    calculate_ngrams(sequences = sequences_solitary, n = y)
    }
  )


```

## Bootstrap N-grams

Here, we bootstrap the data and calculate the conditional probabilities
to check how stable the conditional probabilities are. Set n to 1000

```{r bootstrap_ngram_conditionals}
# Create a bootstraps for all levels
boot_5sec <- 
  bootstrap_n_grams_function(
    observed_n_grams = observed_n_grams_5sec, 
    sequences = sequences_5sec, 
    n_bootstraps = 10)

boot_rapid <-
  bootstrap_n_grams_function(
    observed_n_grams = observed_n_grams_rapid, 
    sequences = sequences_rapid, 
    n_bootstraps = 10)

boot_overlap <-
  bootstrap_n_grams_function(
    observed_n_grams = observed_n_grams_overlap, 
    sequences = sequences_overlap, 
    n_bootstraps = 10)

boot_solitarygestures <-
  bootstrap_n_grams_function(
    observed_n_grams = observed_n_grams_solitarygestures, 
    sequences = sequences_solitary, 
    n_bootstraps = 10)



# Create a list of plots 'plots' using lapply, iterating through 'observed_n_grams'
bootplot_5sec <- create_plots_function(observed_n_grams_5sec, boot_5sec)
bootplot_rapid <- create_plots_function(observed_n_grams_rapid, boot_rapid)
bootplot_overlap <- create_plots_function(observed_n_grams_overlap, boot_overlap)
bootplot_solitarygestures <- create_plots_function(observed_n_grams_solitarygestures, boot_solitarygestures)

```

Now, we plot the distribution of the range of bootstrapped conditional
probabilities, with the frequency of the consequent element on the
x-axis and the probability range on the y-axis.

```{r plots_ngram_boots}

# Arrange and display the plots in a grid
do.call("grid.arrange", c(bootplot_5sec, ncol = 2))  
```

Even for 3-grams (two antecedents predicting third
element) the conditional probabilities are highly unstable. We need to
drop 4-grams, the conditional probabilities are all over the place.

Let's look at the different time intervals together.

```{r plots_compare}

# Arrange and display the plots in a grid
grid.arrange(bootplot_5sec[[1]], bootplot_rapid[[1]], bootplot_overlap[[1]], bootplot_solitarygestures[[1]], ncol = 2)
```


Interestingly, all follow a similar pattern (rare antecedents have unpredictable probabilities, with increasing sample size there is increasing stability), but the inflection points occur at different sample sizes, indicating that, if we had the same sample size, maybe the different would not have similar stability. We can actually check that by testing the re-test reliability of each given different sample sizes.


# Significant N-grams

We establish which gesture transitions are significant by randomising
the gesture tokens, while keeping the gesture probability and the
sequence lengths the same.

## Functions

### Shuffle sequences for significance on different levels

Make a function to shuffle sequences while controlling sequence length.

```{r randomised_sequences}

shuffle_sequences <- function(sequences) {
  # Flatten all sequences into one vector
  all_elements <- unlist(strsplit(sequences, ","))
  
  # Shuffle the elements
  shuffled_elements <-
    sample(all_elements,
           size = length(all_elements),
           replace = FALSE)
  
  # Reassemble the sequences with the same lengths as before
  shuffled_sequences <- rep(NA, length(sequences))
  idx <- 1
  
  # Loop through each sequence
  for (i in 1:length(sequences)) {
    # Calculate the length of the original sequence
    len <- length(strsplit(sequences[i], ",")[[1]])
    
    # Reconstruct the shuffled sequence by pasting the shuffled elements
    shuffled_sequences[i] <-
      paste(shuffled_elements[idx:(idx + len - 1)], collapse = ",")
    
    # Update the index for the next sequence
    idx <- idx + len
  }
  
  return(shuffled_sequences)
}

# Function to generate random n-grams for a given dataset
generate_random_ngrams <- function(observed_ngrams, sequences) {
  lapply(observed_ngrams, function(y) {
    observed_order <- y %>% select(ngram)
    level <- str_count(observed_order$ngram[[1]], ',') + 1
    
    randomised <- lapply(1:100, function(k) {
      random_gram <- calculate_ngrams(
        sequences = shuffle_sequences(sequences), 
        n = level
      )
      
      ordered_random_gram <- left_join(observed_order, random_gram, by = 'ngram') %>% 
        replace_na(list('conditional_probability' = 0)) %>% 
        pull(conditional_probability)
      
    }) %>% bind_cols() %>% suppressMessages()
  })
}


# Function to update observed n-grams with expected values and p-values
update_observed_ngrams <- function(observed_ngrams, random_ngrams) {
  lapply(seq_along(observed_ngrams), function(y) {
    observed_ngrams[[y]] %>% 
      mutate(expected = apply(random_ngrams[[y]], 1, mean)) %>% 
      mutate(pvalue = sapply(1:nrow(observed_ngrams[[y]]), function(k) {
        mean(observed_ngrams[[y]]$conditional_probability[k] <= random_ngrams[[y]][k,])
      })) %>% 
      mutate(level = y + 1)
  })
}
# Function to generate random n-grams for a given dataset
generate_bootstrapped_ngrams <- function(observed_ngrams, sequences) {
  lapply(observed_ngrams, function(y) {
    
    observed_order <- y %>% select(ngram)
    level <- str_count(observed_order$ngram[[1]], ',') + 1
    
    randomised <- lapply(1:10, function(k) {
      
      sequences_for_boot <-
        sequences[sample(1:length(sequences), length(sequences), replace = TRUE)]
    
      random_gram <- calculate_ngrams(
        sequences = sequences_for_boot, 
        n = level
      )
      
      ordered_random_gram <- left_join(observed_order, random_gram, by = 'ngram') %>% 
        replace_na(list('conditional_probability' = 0)) %>% 
        pull(conditional_probability)
      
    }) %>% bind_cols() %>% suppressMessages()
  })
}


process_ngrams <- function(observed_ngrams_list, sequences, random = 'random') {
  if (random == 'random') {
    random_ngrams_list <- generate_random_ngrams(observed_ngrams_list, sequences)
  } else if (random == 'bootstrapped') {
    random_ngrams_list <- generate_bootstrapped_ngrams(observed_ngrams = observed_ngrams_list, sequences = sequences)
  }
  updated_observed_ngrams_list <- update_observed_ngrams(observed_ngrams_list, random_ngrams_list)
  return(updated_observed_ngrams_list)
}

```

### Networks

```{r network_functions}

generate_graph <- function(compare.mat) {
  # Set node weight for the network, set probability of each antecedent
  node.weight <- 
    compare.mat[!duplicated(compare.mat$antecedent), ] %>% 
    mutate(antecedent_probability = antecedent_count/sum(antecedent_count))
  
  trans.mat <- compare.mat
  
  # Prepare data by removing all combinations that do not meet probability and significance standards
  compare.mat <- compare.mat %>%
    filter(count >= 5 & pvalue <= 0.05)
  
  # Set element1 and element2
  compare.mat <- compare.mat %>%
    mutate(element1 = antecedent, element2 = consequent) %>%
    select(element1, element2, conditional_probability)
  
  # Create graph object
  descriptive.graph <- graph_from_data_frame(compare.mat, directed = TRUE, vertices = NULL)
  
  # Set node size
  vertex.attributes(descriptive.graph)$element.prob <- node.weight$antecedent_probability[match(
    vertex.attributes(descriptive.graph)$name,
    node.weight$antecedent
  )]
  
  edge.attributes(descriptive.graph)$weight <- edge.attributes(descriptive.graph)$conditional_probability
  
  # Identify and add missing nodes
  # missing.nodes <- setdiff(node.weight$antecedent, V(descriptive.graph)$name)
  # descriptive.graph <- add_vertices(descriptive.graph, length(missing.nodes), attr = list(name = missing.nodes))
  # 
  net.graph <- descriptive.graph
  
  # Prepare node and edge information
  node.label <- vertex.attributes(net.graph)$name
  node.size <- vertex.attributes(net.graph)$element.prob
  node.size[is.na(node.size)] <- min(node.size, na.rm = TRUE)
  
  edge.weight <- edge.attributes(net.graph)$weight
  edge.size <- cut(edge.weight, 3)
  edge.size.char <- as.character(edge.size)
  edge.size.char[edge.size == levels(edge.size)[1]] <- 1
  edge.size.char[edge.size == levels(edge.size)[2]] <- 3
  edge.size.char[edge.size == levels(edge.size)[3]] <- 5
  edge.size <- as.numeric(edge.size.char)
  if (length(unique(edge.size)) == 1) {
    edge.size <- edge.size / edge.size
  }
  
  net.un <- net.graph
  net.community <- cluster_optimal(net.un)
  modular <- round(modularity(net.community), 2)
  net.com <- data.frame(element = net.community$names, community = net.community$membership)
  color <- rainbow(length(unique(net.com$community)))
  
  vertex.attributes(net.graph)$community <- net.com$community
  
  p <- ggraph(graph = net.graph, layout = "nicely") +
    geom_edge_link(arrow = arrow(length = unit(2, "mm")), colour = "grey", end_cap = circle(3, "mm"),
                   start_cap = circle(2, "mm"), label_dodge = unit(3, "mm"), angle_calc = "along",
                   show.legend = FALSE) +
    geom_node_text(aes(label = .data$name, color = color[net.com$community], size = node.size, fontface = "bold"),
                   show.legend = FALSE) +
    scale_size(range = c(4, 4.1)) +
    ggtitle(paste("Modularity = ", modular, sep = "")) +
    theme_graph(base_family = "sans") +
    geom_edge_loop(aes(span = 20, strength = 0.8), colour = "grey", end_cap = circle(3, "mm"),
                   start_cap = circle(2, "mm"), label_dodge = unit(3, "mm"), angle_calc = "along",
                   show.legend = FALSE)
  
  return(p)
}


```

## Generate significance values

Here, we generate the significance values for the different time frames. We will use the bootstrapped data to check the significance of the observed conditional probabilities.



```{r process_ngrams}
# Applying the function to different datasets
observed_n_grams_5sec <- process_ngrams(observed_n_grams_5sec, sequences_5sec)
observed_n_grams_rapid <- process_ngrams(observed_n_grams_rapid, sequences_rapid)
observed_n_grams_overlap <- process_ngrams(observed_n_grams_overlap, sequences_overlap)
observed_n_grams_solitarygestures <- process_ngrams(observed_n_grams_solitarygestures, sequences_solitary)


```

### List significant ngrams

Make a list of the significant n-grams of all levels for the Full
Sequences at 5 seconds. Change the time frame to see the significance there.

```{r list_significant_ngrams}

# Combine all data frames in the 'observed_n_grams' list into a single data frame
observed_n_grams_5sec %>% 
  bind_rows() %>% 
  # Filter rows where 'pvalue' is less than or equal to 0.05 and 'count' is greater than or equal to 5
  filter(pvalue <= 0.05 & count >= 5) %>% 
  # Arrange the data frame in descending order of 'level'
  arrange(desc(level)) %>% 
  # Select all columns except 'ngram'
  select(-ngram, -antecedent_count, -consequent_count, -specificity) %>% 
  #round all numeric variables to 3 decimal places
  mutate_if(is.numeric, ~ round(., 3)) %>%
  filter(level %in% c(2,3)) %>% 
  select(-level) %>% 
  write.table("clipboard", sep="\t", row.names=FALSE)

```


### Make list of ngrams that are significant across time frames

```{r list_significant_all}

# make one data frame to add all possible significance to
combined_significance <-
  expand.grid(antecedent = unique(
    c(
      observed_n_grams_5sec[[1]]$antecedent,
      observed_n_grams_5sec[[1]]$consequent
    )
  ), consequent = unique(
    c(
      observed_n_grams_5sec[[1]]$antecedent,
      observed_n_grams_5sec[[1]]$consequent
    )
  ))

# for each of the five datasets, select the ones that are significant and occur at least 5 times and add them to the combined_significance data frame
combined_significance <-
  combined_significance %>%
  left_join(
    observed_n_grams_5sec[[1]] %>%
      filter(pvalue <= 0.05 & count >= 5) %>%
      mutate(significance_5sec = 1) %>%
      select(significance_5sec, antecedent, consequent),
    by = c('antecedent', 'consequent')
  ) %>%
  left_join(
    observed_n_grams_rapid[[1]] %>%
      filter(pvalue <= 0.05 & count >= 5) %>%
      mutate(significance_rapid = 1) %>%
      select(significance_rapid, antecedent, consequent),
    by = c('antecedent', 'consequent')
  ) %>%
  left_join(
    observed_n_grams_overlap[[1]] %>%
      filter(pvalue <= 0.05 & count >= 5) %>%
      mutate(significance_overlap = 1) %>%
      select(significance_overlap, antecedent, consequent),
    by = c('antecedent', 'consequent')
  ) %>%
  left_join(
    observed_n_grams_solitarygestures[[1]] %>%
      filter(pvalue <= 0.05 & count >= 5) %>%
      mutate(significance_solitarygestures = 1) %>%
      select(significance_solitarygestures, antecedent, consequent),
    by = c('antecedent', 'consequent')
  ) %>%
  mutate_all( ~ replace_na(., 0))

# count for each row how many of the significance values are 1

combined_significance <-
  combined_significance %>%
  mutate(significance_total = rowSums(select(., starts_with('significance'))))

```


Here is the table with the shared  transitions across all time frames.

```{r table_shared}

combined_significance %>% 
  filter(significance_total == 4) %>% 
  select(antecedent, consequent) %>% 
  kable()

```


Here is what the network of only the shared transitions looks like - we use 3 shared instead of 4 to be less conservative.


```{r shared_transition_network}

# Select only transitions that are significant in all time frames and make a network out of those

net_share <-
  combined_significance %>% 
  filter(significance_total >= 3) %>% 
  select(antecedent, consequent, significance_total) %>% 
  as.matrix() %>% 
  graph_from_data_frame(directed = TRUE) 

net.community <- cluster_optimal(net_share)

vertex.attributes(net_share)$community <- as.factor(net.community$membership)
community_colors <- rainbow(length(unique(net.community$membership)))
vertex.attributes(net_share)$color <- community_colors[net.community$membership]


net_share <- net_share %>% 
  ggraph(layout = 'fr') +
  geom_edge_arc(edge_color = 'grey',
    # edge_colour = "grey", 
    alpha = 0.8, 
    check_overlap = T, 
    edge_width = 2,
    strength = 0.2, 
                arrow = arrow(angle = 30, 
                              length = unit(0.15, "inches"),
                              ends = "last", 
                              type = "open")) +
   geom_edge_loop(edge_color = 'grey', edge_width = 1, edge_linetype = 2) +
  geom_node_point(aes(color = color), size = 3) +
  geom_node_text(aes(label = name, color = color), repel = TRUE, size = 5, vjust = 1.5) +
  theme_void() +
  labs(title = "Concurrence Transition Network") +
  theme(legend.position = "none")

net_share

```

## Networks

```{r network_analysis}

# Apply the function to each observed_n_grams_5sec object and plot

ngram_list <- list(observed_n_grams_overlap[[1]], observed_n_grams_solitarygestures[[1]], observed_n_grams_rapid[[1]], observed_n_grams_5sec[[1]])
names(ngram_list) <- c("overlap", "response_waiting", "1sec", "5sec")

plot_list <- lapply(ngram_list, generate_graph)
plot_list <- lapply(seq_along(plot_list), function(i){plot_list[[i]] + ggtitle(names(ngram_list)[i])})

# Display all plots
do.call(grid.arrange, c(plot_list, ncol = 3))


```


### Rapid response vs solitary gesture network


```{r rapid_response_network}


# Select only transitions that are significant in all time frames and make a network out of those

net_full_prep <-
  combined_significance %>% 
  mutate(rapid_or_reponse = if_else(significance_rapid == 1 & significance_solitarygestures == 1, 'both', 'neither')) %>%
  mutate(rapid_or_reponse = if_else(significance_rapid == 1 & significance_solitarygestures == 0, 'rapid-fire', rapid_or_reponse)) %>%
  mutate(rapid_or_reponse = if_else(significance_rapid == 0 & significance_solitarygestures == 1, 'solitary gestures', rapid_or_reponse)) %>%
  filter(significance_total > 0) %>% 
  filter(rapid_or_reponse != 'neither') %>% 
  select(antecedent, consequent, rapid_or_reponse)

net_full <- 
  net_full_prep %>% 
  select(-rapid_or_reponse) %>%
  as.matrix() %>% 
  graph_from_edgelist(directed = TRUE)

edge.attributes(net_full)$color <- rainbow(4)[as.numeric(as.factor(net_full_prep$rapid_or_reponse))]

edge.attributes(net_full)$rapid_or_reponse <- net_full_prep$rapid_or_reponse

net_share <- net_full %>%  
  ggraph(layout = 'fr') +
  geom_edge_arc(aes(edge_colour = rapid_or_reponse), alpha = 0.8, check_overlap = T, edge_width = 2, strength = 0.2) +
  geom_edge_loop(edge_colour = "grey", edge_width = 1, edge_linetype = 2) +
  geom_node_point(color = "skyblue", size = 3) +
  geom_node_text(aes(label = name), repel = TRUE, size = 5, vjust = 1.5, max.overlaps = 15) +
  theme_bw() +
  labs(title = "Significant Transitions For Rapid-Fire Sequences and Solitary Gesture Sequences") +
  facet_grid(.~rapid_or_reponse, drop = T, scales = 'free') +
  theme(legend.position = "none")

net_share


```

Rapid fire sequences are focused on object-related and contact gestures (stomping, hitting, biting, embracing) while the ones that definitely need response waiting are often those that cannot be physically done too quickly together or those that in a rapid-fire sequence would not be marked as distinct (without response waiting, there would be no way to tease apart Grab, GrabHold, Touch, and Push).


# Significant 2-gram orders

Let's check if there are any combinations where the direction matters
(conditional probability of B following A being more likely than
expected by random distribution of elements within sequences). We
shuffle elements within sequences while keeping sequence lengths the
same. We do this for the 5sec full sequences for now.


## Functions

```{r randomised_within_function}

# Define a function 'shuffle_within_sequences' that shuffles elements within each sequence
shuffle_within_sequences <- function(sequences) {
  # Initialize an empty vector to store the shuffled sequences
  shuffled_sequences <- sapply(sequences, function(sequence) {
    # Split the sequence into individual elements
    elements <- strsplit(sequence, ",")[[1]]
    
    # Shuffle the elements randomly without replacement
    shuffled_elements <- sample(elements, length(elements), replace = FALSE)
    
    # Reassemble the shuffled elements into a sequence
    shuffled_sequence <- paste(shuffled_elements, collapse = ",")
    
    # Return the shuffled sequence
    return(shuffled_sequence)
  })
  
  # Return the vector of shuffled sequences
  return(shuffled_sequences)
}


calculate_pvalues_within <- function(observed_n_grams, sequences, n = 2, num_iterations = 1000) {
  # Extract the observed order of n-grams
  observed_order <- observed_n_grams[[1]] %>% select(ngram)
  
  # Generate random n-grams and calculate conditional probabilities
  randomised_within <- lapply(1:num_iterations, function(k) {
    # Shuffle sequences and calculate n-grams
    random_gram <- calculate_ngrams(
      sequences = shuffle_within_sequences(sequences),
      n = n
    )
    
    # Join the observed order with the bootstrapped n-grams
    ordered_random_gram <- left_join(observed_order, random_gram, by = 'ngram') %>% 
      replace_na(list('conditional_probability' = 0)) %>% 
      pull(conditional_probability)
  }) %>% bind_cols() %>% suppressMessages()
  
  # Extract the y-th element from 'observed_n_grams'
  observed_n_grams[[1]] <- observed_n_grams[[1]] %>% 
    # Calculate the 'expected' value by taking the mean of corresponding values in 'random_n_grams'
    mutate(expected_within = apply(randomised_within, 1, mean)) %>% 
    # Calculate 'pvalue' for each row in the data frame
    mutate(pvalue_within = sapply(1:nrow(observed_n_grams[[1]]), function(k) {
      # Calculate the mean where 'conditional_probability' is less than or equal to values in 'random_n_grams'
      mean(observed_n_grams[[1]]$conditional_probability[k] <= randomised_within[k,])
    }))
  
  # Return the modified observed n-grams with p-values
  return(observed_n_grams[[1]])
}

```

## Generate significance values

```{r randomised_within}

observed_n_grams_5sec[[1]] <- 
  calculate_pvalues_within(observed_n_grams_5sec, 
                           sequences_5sec, 
                           n = 2, 
                           num_iterations = 10)

observed_n_grams_rapid[[1]] <- 
  calculate_pvalues_within(observed_n_grams_rapid, 
                           sequences_rapid, 
                           n = 2, 
                           num_iterations = 10)

observed_n_grams_overlap[[1]] <-
  calculate_pvalues_within(observed_n_grams_overlap, 
                           sequences_overlap, 
                           n = 2, 
                           num_iterations = 10)

observed_n_grams_solitarygestures[[1]] <-
  calculate_pvalues_within(observed_n_grams_solitarygestures, 
                           sequences_solitary, 
                           n = 2, 
                           num_iterations = 10)

```

Make a list of the significant n-grams.

```{r list_significant_within}

# Combine all data frames in the 'observed_n_grams' list into a single data frame
observed_n_grams_5sec[[1]] %>% 

  # Filter rows where 'pvalue' is less than or equal to 0.05 and 'count' is greater than or equal to 5
  filter(pvalue_within <= 0.05 & count >= 5 & antecedent != consequent) %>% 

  # Select all columns except 'ngram'
  select(-ngram) %>% 
  mutate(improvement = conditional_probability - expected_within) %>% 
  arrange(desc(improvement)) %>% 
  select(antecedent, consequent, count, conditional_probability, expected_within, improvement) %>% 

  # Display the data frame in a tabular format using the 'kable' function with the 'simple' format
  kable(format = 'simple')

```

It mainly seems like physical contact (Bite, Pull, Push, Touch) usually
follows rather than precedes other gestures.


# Entropy

## Functions

### Calculate Entropy

This function calculates the entropy of a given sequence.

```{r entropy_function}


ngram_predictor <- function(sequence_variable,
                            n = 1,
                            add_start = T) {

  # create dataset that consist of every sequence of length n+1, with the last element the consequent

  if(add_start){
  # add '[START]' at the end so we can also include the shorter sequences when analysing longer chains
  # split element vector
  elem.split <-
    lapply(
      sequence_variable,
      function(x) {
        str_split(x, pattern = ", ") %>% unlist(F, F)
      }
    )

  # bind elem.split back together with START and END object
  sequence_variable <-
    sapply(
      elem.split,
      function(x) {
        str_c(c(rep("[START]", n - 1), x), collapse = " ")
      }
    )
  }

  # add selected outcome variable
  new_data <-
    data.frame(sequence = sequence_variable %>%
    unlist(),
    rownumber = 1:length(sequence_variable))

  new_data <- new_data %>%
    unnest_tokens(ngram,
      sequence,
      token = "ngrams",
      n = (n + 1),
      to_lower = FALSE
    ) %>%
    drop_na()

  antecedent_sequences <-
    lapply(new_data$ngram, function(x) {
      antecedent <- str_split(x, " ") %>%
        unlist()
      consequent <- antecedent[n + 1]
      antecedent <- antecedent[1:n] %>%
        str_c(collapse = " ")
      return(data.frame(
        antecedent = antecedent,
        consequent = consequent
      ))
    }) %>%
    bind_rows() %>%
    mutate(rownumber = new_data$rownumber) %>%
    filter(consequent != 'NA') %>%
    filter(!(str_detect(antecedent, ' NA') | str_detect(antecedent, 'NA ')))

  return(antecedent_sequences)
}

calculate_entropy <- function(sequences, iterations = 20) {
  lapply(1:iterations, function(y) {
    boot_sequences <- sample(sequences, replace = TRUE)
    
    # Create antecedent and consequent split
    ngram_antecedent <- 
      ngram_predictor(sequence_variable = boot_sequences, n = 2) %>% 
      separate(antecedent, into = c('A', 'B'), sep = ' ', remove = FALSE)
    
    # Remove those with only two elements
    ngram_antecedent_3gram <-
      ngram_antecedent %>% 
      filter(A != 'START') %>% 
      rowwise() %>% 
      mutate(antecedent_sorted = paste(sort(c(A, B)), collapse = ',')) %>% 
      ungroup()
    
    # Create antecedent and consequent split for randomised sequence
    ngram_randomised <- 
      ngram_predictor(sequence_variable = shuffle_sequences(boot_sequences), n = 2) %>% 
      separate(antecedent, into = c('A', 'B'), sep = ' ', remove = FALSE)
    
    ngram_randomised_3gram <-
      ngram_randomised %>% 
      filter(A != 'START') %>% 
      rowwise() %>% 
      mutate(antecedent_sorted = paste(sort(c(A, B)), collapse = ',')) %>% 
      ungroup()
    
    # Calculate entropies for real and shuffled sequences
    entropy_values <- data.frame(
      level = c(0, 1, 2, 2),
      order = c('observed', 'observed', 'observed', 'alphabetical'),
      entropy = c(
        entropy(ngram_antecedent$consequent) %>% natstobits(),
        condentropy(ngram_antecedent$consequent, ngram_antecedent$B) %>% natstobits(),
        condentropy(ngram_antecedent_3gram$consequent, ngram_antecedent_3gram$antecedent) %>% natstobits(),
        condentropy(ngram_antecedent_3gram$consequent, ngram_antecedent_3gram$antecedent_sorted) %>% natstobits()
      ),
      expected_entropy = c(
        entropy(ngram_randomised$consequent) %>% natstobits(),
        condentropy(ngram_randomised$consequent, ngram_randomised$B) %>% natstobits(),
        condentropy(ngram_randomised_3gram$consequent, ngram_randomised_3gram$antecedent) %>% natstobits(),
        condentropy(ngram_randomised_3gram$consequent, ngram_randomised_3gram$antecedent_sorted) %>% natstobits()
      ),
      iteration = y
    )
    
    return(entropy_values)
  })
}


sample_entropy <- function(sequences, iterations = 5, dataset_name = '5sec') {

  k <- rep(seq(1, 0.1, length.out = 10), iterations)
  
  boot_entropies <- lapply(k, function(y){
    
    # split sequences randomly into two equally sized halves
    seq_split1 <- sample(1:length(sequences), (length(sequences)/2) * y)
    sequences_split1 <- sequences[seq_split1]
    
    #calculate entropy
    
    entro <- 
      calculate_entropy(sequences_split1, iterations = 5) %>% 
      bind_rows() %>% 
      filter(order == 'observed') %>%
      filter(level != 0) %>% 
      group_by(order, level) %>% 
      summarise(entropy = mean(entropy), expected_entropy = mean(expected_entropy)) %>%
      ungroup()
      
    
    
    seq_2 <- calculate_ngrams(
      sequences = sequences_split1,
      n = 2
    )
    
    seq_3 <- calculate_ngrams(
      sequences = sequences_split1,
      n = 3
    )
    
    return(
      data.frame(
        cbind(
          entro,
          k = y,
          dataset = dataset_name
        )
      ) %>% 
        mutate(sample_size = if_else(level == 1, sum(seq_2$count), sum(seq_3$count)))
    )
  }) %>% 
    bind_rows() %>% 
    mutate(entropy_ratio = entropy / expected_entropy) %>% 
    suppressMessages()
}
  

entropy_compare_plot <- function(sample_entropy, facet = T, upper_y = 1.1){
  if(facet){
    return(sample_entropy %>% 
    ggplot(aes(x = sample_size, y = entropy_ratio, color = as.factor(dataset))) +
    geom_point(alpha = 0.1) +
    geom_smooth(method = 'loess') +
    geom_hline(yintercept = 1, linetype = 2) +
    theme_classic() +
    xlab('Sample Size') +
    ylab('Entropy Ratio (Observed / Expected)') +
    facet_wrap(level~dataset) + 
    ylim(-0.1,upper_y)+
    ggtitle(sample_entropy$dataset[1]))
  }
  if(!facet){
    return(sample_entropy %>% 
    ggplot(aes(x = sample_size, y = entropy_ratio, color = as.factor(dataset))) +
    geom_point(alpha = 0.1) +
    geom_smooth(method = 'loess') +
    geom_hline(yintercept = 1, linetype = 2) +
    theme_classic() +
    xlab('Sample Size') +
    ylab('Entropy Ratio (Observed / Expected)') +
    ylim(-0.1,upper_y)+
    ggtitle(sample_entropy$dataset[1]))
  }
  
}

```


## Entropies all datasets

Calculate the entropies across all 5 datasets.

```{r conditional_entropies_all}

bootstrapped_entropy_5sec <- calculate_entropy(sequences_5sec, iterations = 10)
bootstrapped_entropy_rapid <- calculate_entropy(sequences_rapid, iterations = 10)
bootstrapped_entropy_solitarygestures <- calculate_entropy(sequences_solitary, iterations = 10)
bootstrapped_entropy_overlap <- calculate_entropy(sequences_overlap, iterations = 10)

```

Let's plot this.

```{r entropy_plot}

entropy_values <- bootstrapped_entropy_5sec %>% bind_rows()

entropy_values %>% 
  select(level, order, entropy,iteration) %>% 
  mutate(type = 'observed') %>% 
  rbind(entropy_values %>% 
          mutate(entropy = expected_entropy) %>% 
          select(level, order, entropy,iteration) %>% 
          mutate(type = 'expected_random')) %>% 
  mutate(type_iteration = str_c(type, order, iteration, sep = '_')) %>% 
  # rbind(entropy_values %>% 
  #         mutate(entropy = expected_entropy_controlled) %>% 
  #         select(level, order, entropy,iteration) %>% 
  #         mutate(type = 'expected_controlled')) %>%
  filter(order!='alphabetical') %>% 
  ggplot(aes(x = as.factor(level), y = entropy, color = type, shape = type,
                       group = as.factor(type_iteration))) + 
  geom_point(size = 2, alpha = 0.2) + 
  geom_line(linetype = 1) + 
  geom_point(data = entropy_values %>% 
               select(level, order, entropy,iteration) %>% 
               mutate(type = 'observed') %>% 
               rbind(entropy_values %>% 
                       mutate(entropy = expected_entropy) %>% 
                       select(level, order, entropy,iteration) %>% 
                       mutate(type = 'expected_random')) %>% 
               mutate(type_iteration = str_c(type, order, iteration, sep = '_')) %>% 
               filter(order=='alphabetical'),
             aes(x = as.factor(level), 
                 y = entropy, shape = type), size = 2, alpha = 0.2, 
                 color = 'orange',position = position_nudge(x = 0.3)) +
  theme_classic() +
  ylim(0,5) + 
  xlab('Antecedent Length') + 
  ylab('Entropy in Bits')

```

Doted lines are predicted for random, lower values mean 'less chaotic'.
From the entropies, there is considerable improvement in predictability
moving from no antecedent to one antecedent, but not from 1 antecedent
to 2 antecedents. There seems a slight improvement of having 2
antecedent when the order is removed ('alphabetical'), indicating the
order just adds noise.

Let's plot the ratio between expected and observed entropies. Anything
closer to 1 or above means randomness, anything closer to 0 means more
predictability.

```{r entropy_plot_rapid}

entropy_value_comparison <-
  bootstrapped_entropy_5sec %>% 
  bind_rows() %>% 
  mutate(dataset = '5sec') %>%
  bind_rows(
    bootstrapped_entropy_overlap %>% bind_rows() %>% 
      mutate(dataset = 'overlap'))  %>%
  bind_rows(
    bootstrapped_entropy_rapid %>% bind_rows() %>% 
      mutate(dataset = 'rapid-fire')) %>%
  bind_rows(
    bootstrapped_entropy_solitarygestures %>% bind_rows() %>% 
      mutate(dataset = 'solitarygestures')) %>%
  
  mutate(entropy_ratio = entropy / expected_entropy) %>% 
  filter(order == 'observed') %>%
  ggplot(aes(x = as.factor(level), y = entropy_ratio, color = dataset)) +
  geom_jitter(size = 2, alpha = 0.3) +
  ylim(0, 2) +
  geom_hline(yintercept = 1, linetype = 2) +
  theme_classic() +
  facet_wrap(~dataset, scales = 'free_y') +
  xlab('Antecedent Length') +
  ylab('Entropy Ratio (Observed / Expected)')


```

```{r entropy_value_comparison, fig.width=10, fig.height=5}
entropy_value_comparison
```

For level 0 and 1 (base probabilities and 1-element antecedents), the
different sequence types are the same apart from the Overlap sequences,
which are more predictable at Level 1 (which mirrors our results above,
there are simply fewer options). For 2-antecedent transitions, the
5-second sequence definition adds the most predictability about what
happens next; in 1-second sequences and overlap, there is no added
information above random (most of them aren't even visible in this
chart); the solitary gesture (which should be
fairly similar anyways) add a bit of information, but on average less
than the 5sec sequences.

### Impact of sample size on entropies

The different datasets have different sample sizes. Let's check how the sample size impacts the entropy.


```{r entro_sample_compare}

sample_entropy_5sec <- sample_entropy(sequences_5sec, dataset_name = '5sec', iterations = 5)
sample_entropy_rapid <- sample_entropy(sequences_rapid, dataset_name = 'rapid-fire', iterations = 5)
sample_entropy_solitarygestures <- sample_entropy(sequences_solitary, dataset_name = 'solitarygestures', iterations = 5)
sample_entropy_overlap <- sample_entropy(sequences_overlap, dataset_name = 'overlap', iterations = 5)

```


For the first level entropy, this looks like this: essentially, all datasets have sufficient data to accurately calculate the entropy inherent in the first order transitions, and they do not differ a lot (apart from overlap potentially having less entropy compared to predicted). The y-axis is the ratio between the observed and expected entropy values.

```{r entropy_sample_plot_l1, fig.width=10, fig.height=5}

entropy_compare_plot(rbind(sample_entropy_5sec, sample_entropy_rapid, sample_entropy_solitarygestures, sample_entropy_overlap) %>% filter(level == 1), facet = F, upper_y = 1.2) + ylim(0,1) + ggtitle('Entropy Ratio by Sample Size') + labs(color='dataset') 

```


We make a table of the different expected and observed entropies for the full dataset for each:

```{r entropy_table}

rbind(sample_entropy_5sec, sample_entropy_rapid, sample_entropy_solitarygestures, sample_entropy_overlap) %>% 
  filter(k == 1) %>% 
  group_by(order, level, dataset) %>% 
  summarise(entropy = mean(entropy), expected_entropy = mean(expected_entropy)) %>%
  ungroup() %>% 
  filter(level == 1) %>% 
  select(dataset, level, order, entropy, expected_entropy) %>% 
  mutate(entropy_ratio = entropy / expected_entropy) %>%
  kable()

```


# Prediction Models

## Functions

```{r prediction_models}
# Define functions

process_sequences <- function(sequences, slice_prop = 1) {
  ngram_predictor(sequence_variable = sequences, 
                  n = 1, add_start = FALSE) %>% 
    add_count(antecedent, name = 'n') %>%
    slice_sample(prop = slice_prop)
}

split_dataset <- function(dataset, k = 10) {
  training_split(dataset, k = k)
}

assign_outcome <- function(dataset, split_set) {
  assign_training(outcome_variable = dataset$consequent,
                  fold_list = split_set,
                  as_integer = FALSE)
}

transform_input <- function(split_set, dataset) {
  sequence_pad(
    fold_list = split_set,
    sequence_variable = dataset$antecedent,
    n = 1
  )$return_list
}

create_nb_model <- function(input, outcome) {
  base_classifiers(input = input,
                   outcome = outcome, upsample = TRUE)
}

base_classifiers <- function(input,
                     outcome,
                     architecture = 'nb',
                     upsample = TRUE,
                     inverted = FALSE,
                     randomised = FALSE) {
  # Create a list to store the predictions
  nb_prediction <- lapply(seq_along(outcome), function(i){

    # set training and test data
    # Convert the training data of the i-th fold to a data frame with factor variables
    input_train <- input[[i]]$training %>%
      as.data.frame() %>%
      mutate(across(everything(), as.factor))

    # Convert the testing data of the i-th fold to a data frame with factor variables
    input_test <- input[[i]]$testing %>%
      as.data.frame() %>%
      mutate(across(everything(), as.factor))

    # Rename the columns of the training and testing data frames to 'X1', 'X2', ...
    colnames(input_train) = NULL
    colnames(input_test) = NULL

    # Remove any predictors in the testing data that are not present in the training data, and adjust the outcome variable accordingly
    length_test <- nrow(input_test)
    rem_test <- sapply(1:ncol(input_train), function(k){
      which(!(input_test[,k] %in% input_train[,k]))
    }) %>% unlist() %>% unique()

    outcome_train <- outcome[[i]]$training %>% as.character()
    outcome_test <- outcome[[i]]$testing %>% as.character()

    input_test <- input_test[!(1:nrow(input_test) %in% rem_test),] %>% as.data.frame()
    outcome_test <- outcome_test[!(1:length(outcome_test) %in% rem_test)]

    colnames(input_train) <- colnames(input_test) <- str_c('X', 1:ncol(input_train), sep = '')

    # Create a data frame for the training predictors, with the outcome variable as the first column
    pred_train <- data.frame(
      outcome = as.factor(outcome_train)
    )
    pred_train <- cbind(pred_train, input_train)

    # Create a data frame for the testing predictors, with the outcome variable as the first column
    pred_test <- data.frame(
      outcome = as.factor(outcome_test)
    )
    pred_test <- cbind(pred_test, input_test)

    # Balance the training dataset by oversampling the underrepresented classes

    if (upsample){
      # Create recipe to preprocess data
      rec <- recipe(outcome ~ ., data = pred_train) %>%
        themis::step_upsample(outcome, over_ratio = 1, skip = FALSE) %>%
        step_unknown(all_predictors())

      # Fit the recipe to the training data
      pred_train <- prep(rec) %>% juice() %>% data.frame()
    }


    # Align factor levels between the training and testing datasets
    both_levels <- function(x, y) {
      union(levels(x %>% as.factor), levels(y %>% as.factor))
    }

    pred_train <- pred_train %>% mutate(across(everything(), as.character))
    pred_test <- pred_test %>% mutate(across(everything(), as.character))

    pred_both <- rbind(pred_train, pred_test) %>% mutate(across(everything(), as.factor))
    out_both <- pred_both %>% pull(outcome)
    pred_both <- pred_both %>% select(-outcome) %>% mutate(outcome = out_both)

    pred_train <- pred_both[1:nrow(pred_train),]
    pred_test <- pred_both[(nrow(pred_train)+1):nrow(pred_both),]

    # If the `inverted` flag is set, reverse the order of columns in the training and testing data frames
    if(inverted){
      outcome_train <- pred_train %>% pull(outcome)
      outcome_test <- pred_test %>% pull(outcome)
      pred_train <- pred_train %>%
        select(-outcome) %>%
        data.frame() %>%
        rowwise() %>%
        rev() %>%
        ungroup() %>%
        mutate(outcome = outcome_train)

      pred_test <- pred_test %>%
        select(-outcome) %>%
        data.frame() %>%
        rowwise() %>%
        rev() %>%
        ungroup() %>%
        mutate(outcome = outcome_test)
    }

    # If the `randomised` flag is set, shuffle the rows of the training and testing data frames
    if(randomised){
      outcome_train <- pred_train %>% pull(outcome)
      outcome_test <- pred_test %>% pull(outcome)
      input_train2 <- pred_train %>%
        select(-outcome)
      input_test2 <- pred_test %>%
        select(-outcome)

      pred_train <- lapply(1:nrow(pred_train), function(x){
        data.frame(t(sort(as.character(input_train2[x,]))))
      }) %>%
        bind_rows() %>%
        as.matrix()  %>%
        data.frame() %>%
        mutate(outcome = outcome_train)
      colnames(pred_train) <- colnames(pred_both)

      pred_test <- lapply(1:nrow(pred_test), function(x){
        data.frame(t(sort(as.character(input_test2[x,]))))
      }) %>%
        bind_rows() %>%
        as.matrix()  %>%
        data.frame() %>%
        mutate(outcome = outcome_test)
      colnames(pred_test) <- colnames(pred_both)
    }

    # Remove rows in the test dataset where the test dataset has levels that are not part of the training
    for(i in seq_along(pred_test)){
      pred_test[!(pred_test[,i] %in% pred_train[,i]), i] = NA
    }

    pred_test <- pred_test %>% na.omit()

    # turn outcomes into factors

    pred_train$outcome <- as.factor(pred_train$outcome)
    pred_test$outcome <- as.factor(pred_test$outcome)

    # Function to tune hyperparameters

    classifier  <- switch(architecture,
                          "nb" = naive_Bayes(Laplace = 0.01, engine = 'naivebayes'),
                          "mlp" = mlp(hidden_units = 24, dropout = 0.01, epochs = 30, mode = 'classification') %>% set_engine('nnet', MaxNWts = 10000000),
                          "forest" = rand_forest(mtry = 3, trees = 1000) %>%
                            set_engine("ranger", importance = "impurity", num_threads = 10),
                          "svm" = svm_poly(engine = "kernlab"),
                          "boost" = boost_tree(engine = 'xgboost', tree_depth = 2),
                          "C5" =  C5_rules(engine = 'C5.0'),
                          "log_reg" = logistic_reg(penalty = 1, mixture = 1, engine = "glmnet")
    )

    # Define workflow
    classifier_wf <-  workflow() %>%
      add_model(classifier %>%
      set_mode("classification")) %>%
      add_formula(outcome ~ .) %>%
      step_naomit()


    # Fit a machine learning classifier model to the training data
    classifier_fit  <- classifier_wf %>%
      parsnip::fit(pred_train) %>%
      suppressWarnings()

    # Use the trained model to predict outcomes for the test data
    # Add a column for the true outcomes from the test data
    # Convert the predicted outcomes and true outcomes to factors
    # Suppress warnings that may arise during the conversion
    model.results <- predict(classifier_fit, pred_test) %>%
      bind_cols(target = as.factor(as.character(pred_test$outcome))) %>%
      mutate(.pred_class = as.factor(as.character(.pred_class))) %>%
      suppressWarnings()

    # Calculate the accuracy of the model by counting the number of correct predictions
    # (where the predicted class is the same as the target class) and dividing by the total number of test samples
    accuracy.nb <- model.results %>%
      filter(as.character(target) == (.pred_class)) %>%
      nrow() / length_test

    # Predict class probabilities
    class_probs <- predict(classifier_fit, new_data = pred_test, type = "prob")
    colnames(class_probs) <- colnames(class_probs) %>% str_remove_all('.pred_')

    # if(nlevels(pred_test$outcome) == 2){
    #   class_probs_est <- unlist(sapply(seq_along(pred_test$outcome), function(y) class_probs[y, which(levels(pred_test$outcome) == as.character(pred_test$outcome[y]))]))
    #   roc_auc_val <- roc_auc_vec(truth = as.factor(pred_test$outcome), estimate = class_probs_est)
    # } else {
    #   roc_auc_val <- roc_auc_vec(truth = as.factor(pred_test$outcome), estimate = class_probs %>% as.matrix())
    # }
    # Calculate the ROC and AUC

    # Calculate the log-likelihood
    predicted_probs <- sapply(1:nrow(class_probs), function(x){class_probs[x, pred_test$outcome]}) %>% unlist()
    log_likelihood <- sum(log(predicted_probs)[!is.infinite(log(predicted_probs))], na.rm = T)/nrow(pred_test)

    # Save the predicted and expected outcomes for further analysis
    observed.sampled <- model.results$.pred_class
    expected <- as.character(pred_test$outcome)

    return(list(
      accuracy.sampled = accuracy.nb,
      sample.size = nrow(input_train),
      observed.sampled = as.character(observed.sampled),
      expected = expected,
     # roc_auc = roc_auc_val,
      log_likelihood = log_likelihood
    ))
  })

  # Transpose the prediction results
  nb_pred <- purrr::transpose(nb_prediction)

  # Create a data frame with predicted and expected values
  plot.exp <- data.frame(
    predicted = unlist(nb_pred$observed.sampled),
    expected = unlist(nb_pred$expected)
  )

  element_correct <-
    plot.exp %>%
    mutate(correct = predicted == expected) %>%
    group_by(expected) %>%
    summarise(accuracy = mean(correct),
              count = n()) %>%
    ungroup()

  # Set levels for both predicted and expected variables
  both_levels <- union(levels(as.factor(plot.exp$predicted)), levels(as.factor(plot.exp$expected)))
  plot.exp$predicted <- factor(plot.exp$predicted, levels = both_levels)
  plot.exp$expected <- factor(plot.exp$expected, levels = both_levels)

  # Create a confusion matrix plot
  x.plot <- plot.exp %>%
    conf_mat(expected, predicted) %>%
    autoplot(type = "heatmap") +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.6))

  # Create a data frame for misclassification probabilities
  misclass <- data.frame(table(plot.exp$expected, plot.exp$predicted))
  colnames(misclass) <- c("observed", "expected", "count")
  tot.exp <- misclass %>%
    group_by(expected) %>%
    summarise(tot.exp = sum(count))

  # Calculate misclassification probabilities and remove unnecessary column
  misclass <- misclass %>%
    left_join(tot.exp) %>%
    mutate(misclassification.probability = round(count / tot.exp, 3)) %>%
    select(-tot.exp) %>%
    suppressMessages()

  # Create a confusion matrix as a matrix and add probabilities as edge attributes
  mis.matr <- graph_from_edgelist(as.matrix(misclass[, c("expected", "observed")]), directed = TRUE)
  edge.attributes(mis.matr)$probability <- misclass$misclassification.probability
  mis.matr <- mis.matr %>%
    get.adjacency(attr = "probability") %>%
    as.matrix()

  # Create a heatmap of the misclassification probabilities
  mis.plot <- ggplot(
    data = misclass,
    aes(x = expected, y = observed, fill = misclassification.probability)
  ) +
    geom_tile() +
    scale_fill_gradient2(
      low = "white",
      high = "red",
      limit = c(0, max(misclass$misclassification.probability)),
      space = "Lab",
      name = "Confusion Probability"
    ) +
    theme_minimal() +
    xlab("expected") +
    ylab("observed") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1)) +
    ggtitle(paste("Misclassification Heat Map, Level =", ncol(input[[1]]$training)))

  # run model with full dataset to get embeddings

  # create a data frame of the full input
  if (ncol(input[[1]]$training %>% as.matrix()) == 1) {
    input_full <- as.character(c(input[[1]]$training, input[[1]]$testing) %>% unlist())
  } else {
    input_full <- do.call(rbind, input[[1]]) %>%
      data.frame() %>%
      mutate(across(everything(), as.character))
  }

  # create a data frame of the full prediction
  pred_full <- data.frame(outcome = factor(c(outcome[[1]]$training, outcome[[1]]$testing)))

  # combine the full prediction and input data frames
  pred_full <- cbind(pred_full, input_full)

  if (upsample){
    # Create recipe to preprocess data
    rec <- recipe(outcome ~ ., data = pred_full) %>%
      themis::step_upsample(outcome, over_ratio = 1, skip = FALSE)

    # Fit the recipe to the training data
    pred_full <- prep(rec) %>% juice() %>% data.frame()
  }

  # If the `inverted` flag is set, reverse the order of columns in the training and testing data frames
  if(inverted){
    outcome_full <- pred_full %>% pull(outcome)
    input_full2 <- pred_full %>%
      select(-outcome)

    pred_full <- lapply(1:nrow(pred_full), function(x){
      data.frame(t(sort(as.character(input_full2[x,]))))
    }) %>%
      bind_rows() %>%
      as.matrix()  %>%
      data.frame() %>%
      mutate(outcome = outcome_full)
    colnames(pred_full) <- c(colnames(input_full2),'outcome')
  }

  # If the `randomised` flag is set, shuffle the rows of the training and testing data frames
  if(randomised){
    outcome_full <- pred_full %>% pull(outcome)

    pred_full <- pred_full %>%
      select(-outcome) %>%
      data.frame() %>%
      rowwise() %>%
      sample() %>%
      ungroup() %>%
      mutate(outcome = outcome_full)
  }

  classifier  <- switch(architecture,
                        "nb" = naive_Bayes(Laplace = 0.01, engine = 'naivebayes'),
                        "mlp" = mlp(hidden_units = 12, dropout = 0.1, epochs = 50, engine = 'keras', verbose = FALSE),
                        "forest" = rand_forest(mtry = 3, trees = 1000, engine = 'ranger'),
                        "svm" = svm_poly(engine = "kernlab"),
                        "boost" = boost_tree(engine = 'xgboost'),
                        "C5" =  C5_rules(engine = 'C5.0'),
                        "log_reg" = logistic_reg(penalty = 1, mixture = 1, engine = "glmnet"),
                        "multinom_reg" = multinom_reg(penalty = 1, mixture = 1, engine = "glmnet")
  )

  # Define workflow
  classifier_wf <-  workflow() %>%
    add_model(classifier %>%
                set_mode("classification")) %>%
    add_formula(outcome ~ .) %>%
    step_naomit()


  # Fit a machine learning classifier model to the training data
  classifier_fit  <- classifier_wf %>% parsnip::fit(pred_full) %>% suppressWarnings()

  return(
    list(
      full_model = extract_fit_parsnip(classifier_fit),
      element_correct = element_correct,
      accuracy = mean(unlist(nb_pred$accuracy.sampled), na.rm = T),
      accuracy_distribution = unlist(nb_pred$accuracy.sampled),
      roc_auc = NA,
      roc_auc_distribution = NA,
      log_likelihood = mean(unlist(nb_pred$log_likelihood), na.rm = T),
      log_likelihood_distribution = unlist(nb_pred$log_likelihood),
      sample_size = mean(unlist(nb_pred$sample.size), na.rm = T),
      confusion.matrix = x.plot,
      heatmap = mis.plot
    )
  )
}

training_split <- function(sequence_object,
                           k = 1) {

  # if k == 1, split 80/20 into training and test set

  if (k == 1) {
    so.kfold <- initial_split(sequence_object, prop = 8 / 10)
  }
  # if k > 1, split into that many folds
  if (k > 1) {
    so.kfold <- vfold_cv(sequence_object, v = k)
  }
  return(so.kfold)
}

assign_training <- function(outcome_variable,
                            fold_list,
                            as_integer = TRUE) {
  # if fold_list = NULL, create training set with all data

  if (is.null(fold_list)) {
    in_id <- list(1:length(outcome_variable))
  }

  if (!is.null(fold_list)) {
    in_id <- lapply(fold_list$splits, function(x) {
      x$in_id
    })
  }

  if (outcome_variable %>% is.vector()) {
    # select sequence variable
    new_data <- outcome_variable %>% unlist()

    if (as_integer) {
      new_data <- as.numeric(as.factor(new_data)) - 1
    }
  }

  if (!(outcome_variable %>% is.vector())) {
    new_data <- outcome_variable %>% data.frame()
  }
  # add tokenized data to splits

  return_list <- lapply(in_id, function(i) {
    # add outcome data framen
    if (outcome_variable %>% is.vector()) {
      seq_training <- new_data[i]
      seq_testing <- new_data[!(1:length(new_data) %in% i)]
    }

    if (!(outcome_variable %>% is.vector())) {
      seq_training <- new_data[i, ]
      seq_testing <- new_data[!(1:nrow(new_data) %in% i), ]
    }

    return(list(training = seq_training,
                testing = seq_testing))
  })

  return(return_list)
}
sequence_pad <- function(sequence_variable,
                         fold_list,
                         n = 1) {

  # if fold_list = NULL, create training set with all data

  if(is.null(fold_list)){
    in_id <- list(1:length(sequence_variable))
  }

  if(!is.null(fold_list)){
    in_id <- lapply(fold_list$splits, function(x){x$in_id})
  }

  # select sequence variable
  new_data <- sequence_variable %>%
    unlist() %>%
    str_replace_all('\\.', replacement = '') %>%
    str_replace_all('START ', '') %>%
    str_replace_all('START', '') %>%
    str_replace_all(',', '')

  # create tokenizer
  seq_tokenizer <- text_tokenizer(1000000,
    lower = FALSE,
    split = " ",
    char_level = FALSE
  )

  # apply tokenizer to sequences
  fit_text_tokenizer(seq_tokenizer, new_data)
  new_data <- texts_to_sequences(seq_tokenizer, new_data)

  new_data <- new_data %>%
    pad_sequences(
      maxlen = n,
      padding = "post",
      value = 0
    )

  # add tokenized data to splits

  return_list <- lapply(in_id, function(i){

    # add outcome data framen
    seq_training <- new_data[i,]
    seq_testing <- new_data[!(1:nrow(new_data) %in% i),]

    return(list(training = seq_training,
                testing = seq_testing))
  })

  return(list(return_list = return_list,
         tokenizer = seq_tokenizer))
}

```



### Comparison of predictability of different datasets

```{r markov_predictions}

# Set the number of bootstraps; in paper, it's 10
number_bootstraps <- 5
# Set the sample size splitting (how finely should data be split); in paper is 0.1
sample_size_split <- 0.2

# Process each dataset
datasets <- list(sequences_overlap, sequences_rapid, sequences_solitary, sequences_5sec)
names(datasets) <- c("overlap", "rapid-fire", "solitary_gestures", "5sec")

results <- lapply(seq_along(datasets), function(k) {
  sequences <- datasets[[k]]
  boot_res <- lapply(rep(seq(0.4, 1, by = sample_size_split), number_bootstraps), function(props){
    ngram_antecedent <- process_sequences(sequences, slice_prop = props)
    split_set <- split_dataset(ngram_antecedent)
    outcome <- assign_outcome(ngram_antecedent, split_set)
    input <- transform_input(split_set, ngram_antecedent)
  
    nb_model <- create_nb_model(input, outcome)
    
    return(
      data.frame(sample_size = nrow(ngram_antecedent),
            dataset = names(datasets)[k],
            nb_accuracy = nb_model$accuracy)
    )
  }) %>% bind_rows()
})

names(results) <- names(datasets)

# Combine and plot results
plotting_results <- do.call(bind_rows, results)

plotting_results <- 
  bind_rows(
    plotting_results %>% 
      mutate(accuracy = nb_accuracy) %>% 
      mutate(model = 'NB') %>%
      select(sample_size, dataset, model, accuracy)
            )

p.accuracy_predictions <- 
  ggplot(plotting_results, aes(x = sample_size, y = accuracy, color = dataset)) +
  geom_point(alpha = 0.3, size = 3) +
  geom_smooth(method = 'loess', se = T) +
  ylim(0, 1) +
  theme_classic() +
  ggtitle('Accuracy Predict Next Element')

# Print plot
print(p.accuracy_predictions)

```


## Higher-order predictions

### Data preparation

Let's check how predictable transitions are on different levels. We use
Naive Bayes classifier (which is agnostic to sequence information) and Random Forests to predict the next element based on the previous one. Originally, this also involved LSTM, but Long-Short Term Memory deep learning (which represents sequence
information), but the overfitting was so extreme that it is hard to interpret the results. We look at 3-grams: AB predicting C. We check whether A
alone (one gesture removed), B alone (previous gesture), AB together, or
a alphabetisation of AB best predicts the next element.

```{r next_element_sequence_prep}

# split again as before, but with the sequences that have removed rare elements

ngram_antecedent_3gram <-
  ngram_predictor(sequence_variable = sequences_5sec, 
                  n = 2, add_start = TRUE) %>% 
  # separate antecedent
  separate(antecedent, into = c('A', 'B'), sep = ' ', remove = FALSE) %>% 
  filter(A != 'START') 
  

# # set split into training and test data

split_set_3gram <- 
  training_split(ngram_antecedent_3gram,
                 k = 10)

# set outcome variable

outcome_3gram <- 
  assign_training(outcome_variable = ngram_antecedent_3gram$consequent,
                  fold_list = split_set_3gram,
                  as_integer = FALSE)

# transform input to padded sequence

input_3gram <- sequence_pad(
  fold_list = split_set_3gram,
  sequence_variable = ngram_antecedent_3gram$antecedent,
  n = 2)


# save tokenizer
seq_tok <- input_3gram$tokenizer
input_all <- input_3gram$return_list  

input_A <- lapply(input_all, function(a){
    aa <- lapply(1:2, function(b){
      a[[b]][,1]
    })
    names(aa) <- c('training', 'testing')
    return(aa)
  })

input_B <- lapply(input_all, function(a){
    aa <- lapply(1:2, function(b){
      a[[b]][,2]
    })
    names(aa) <- c('training', 'testing')
    return(aa)
  })


input_A <- lapply(input_A, function(a){
  outlier <- setdiff(1:(input_A %>% unlist() %>% max()), input_A %>% 
                                unlist() %>% 
                                table %>% 
                                names)
    aa <- lapply(1:2, function(b){
      bb <- a[[b]]
      bb[bb > min(outlier)] = 
        bb[bb > min(outlier)] - 1
      return(bb)
    })
    names(aa) <- c('training', 'testing')
    return(aa)
  })

input_B <- lapply(input_B, function(a){
  outlier <- setdiff(1:(input_B %>% unlist() %>% max()), input_B %>% 
                                unlist() %>% 
                                table %>% 
                                names)
    aa <- lapply(1:2, function(b){
      bb <- a[[b]]
      bb[bb > min(outlier)] = 
        bb[bb > min(outlier)] - 1
      return(bb)
    })
    names(aa) <- c('training', 'testing')
    return(aa)
  })
```


### Sequence Prediction

```{r next_element_sequence_prediction, cache.lazy=FALSE}

nb_A <- 
  base_classifiers(input = input_A,
                   outcome = outcome_3gram, upsample = TRUE)

nb_B <- 
  base_classifiers(input = input_B,
                   outcome = outcome_3gram, upsample = TRUE)

nb_AB <- 
  base_classifiers(input = input_all,
                   outcome = outcome_3gram, upsample = TRUE)

nb_AB_randomised <- 
  base_classifiers(input = input_all,
                   outcome = outcome_3gram, 
                   randomised = TRUE, 
                   upsample = TRUE)

```

```{r next_element_sequence_plot}

plotting <- bind_rows(
  data.frame(predictor = 'A', 
             method = 'NB', 
             log_likelihood = nb_A$log_likelihood_distribution, 
             accuracy = nb_A$accuracy_distribution, 
             roc_auc = nb_A$roc_auc_distribution),
  data.frame(predictor = 'B', 
             method = 'NB', 
             log_likelihood = nb_B$log_likelihood_distribution, 
             accuracy = nb_B$accuracy_distribution, 
             roc_auc = nb_B$roc_auc_distribution),
  data.frame(predictor = 'AB', 
             method = 'NB', 
             log_likelihood = nb_AB$log_likelihood_distribution, 
             accuracy = nb_AB$accuracy_distribution, 
             roc_auc = nb_AB$roc_auc_distribution),
  data.frame(predictor = 'rBA', 
             method = 'NB', 
             log_likelihood = nb_AB_randomised$log_likelihood_distribution, 
             accuracy = nb_AB_randomised$accuracy_distribution, 
             roc_auc = nb_AB_randomised$roc_auc_distribution)
) %>% mutate(iteration = rep(1:length(nb_AB$accuracy_distribution), 4))

xxa <- aggregate(plotting$accuracy, by = list(plotting$predictor, plotting$method), mean)
colnames(xxa) <- c('predictor','method','accuracy')
xxb <- aggregate(plotting$log_likelihood, by = list(plotting$predictor, plotting$method), mean)
colnames(xxa) <- c('predictor','method','accuracy')
colnames(xxb) <- c('predictor','method','log_likelihood')

plotting$predictor <- factor(plotting$predictor, levels = c('A' , 'B', 'AB', 'rBA'))

p.accuracy <- ggplot(plotting,
                     aes(
                       x = predictor,
                       y = accuracy,
                       group = as.factor(iteration)
                     )) + geom_crossbar(
                       data = xxa,
                       aes(
                         x = predictor,
                         ymin = accuracy,
                         ymax = accuracy,
                         y = accuracy,
                         group = predictor
                       ),
                       width = 0.5,
                       color = 'red'
                     ) +
  geom_point(alpha = 0.3, size = 3) +
  geom_line(linetype = 2) +
  ylim(0, 1) +
  theme_classic() +
  facet_wrap( ~ method) +
  ggtitle('Accuracy Predict Next Element')

p_test_accuracy <-
  bind_rows(plotting %>% 
              filter(method == 'NB') %>% 
              pairwise_t_test(accuracy ~ predictor, paired = T,
                              p.adjust.method = 'fdr') %>% 
              mutate(method = 'NB') %>% 
              select(method, group1, group2, p.adj,p.adj.signif) %>% 
              mutate(p.adj = round(p.adj, 4))) %>% 
  mutate(predictor1 = group1, predictor2 = group2) %>% 
  select(-group1, -group2)


p.llh <- ggplot(plotting,
                     aes(
                       x = predictor,
                       y = log_likelihood,
                       group = as.factor(iteration)
                     )) + geom_crossbar(
                       data = xxb,
                       aes(
                         x = predictor,
                         ymin = log_likelihood,
                         ymax = log_likelihood,
                         y = log_likelihood,
                         group = predictor
                       ),
                       width = 0.5,
                       color = 'red'
                     ) +
  geom_point(alpha = 0.3, size = 3) +
  geom_line(linetype = 2) +
  theme_classic() +
  facet_wrap( ~ method) +
  ggtitle('Log Likelihood Predict Next Element')

p.roc <- ggplot(plotting, 
       aes(x = as.factor(method), y = roc_auc, fill = predictor, color = predictor, shape = predictor)) +  
  geom_point(alpha = 0.8, size = 3) + 
  theme_classic() +
  ggtitle('ROC Predict Next Element')


```

```{r plot_accuracy, fig.width=10, fig.height=5}
p.accuracy
```

